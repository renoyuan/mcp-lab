import asyncio
import re
import json
from fastapi import FastAPI, Request, WebSocket
from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse
from mcp.client.sse import sse_client
from mcp.client.stdio import stdio_client
from langchain_mcp_adapters.tools import load_mcp_tools
from openai import OpenAI
from openai import AsyncOpenAI 

import asyncio
import json

from openai import OpenAI,AsyncOpenAI

from fastapi import FastAPI, Request, WebSocket
from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse

from mcp.client.session import ClientSession
from mcp.client.stdio import stdio_client
from mcp import StdioServerParameters

import tomllib as tomli  # Python < 3.11
# æˆ–ä½¿ç”¨å†…ç½®åº“ï¼ˆPython â‰¥ 3.11ï¼‰: import tomllib as tomli

with open("config.toml", "rb") as f:  # äºŒè¿›åˆ¶æ¨¡å¼é¿å…ç¼–ç é—®é¢˜
    cfg = tomli.load(f)
    
base_url = cfg["global"]["base_url"] 
api_key = cfg["global"]["api_key"] 
ollama_llm = OpenAI(
    base_url=base_url,  # ä¿®æ­£Ollama APIè·¯å¾„
    api_key=api_key  # è®¤è¯å ä½ç¬¦
)
# æœ¬åœ°åŠ è½½ mcp-tools
server_params =StdioServerParameters(
    command="python",
    args=["mcp_server.py"],
)





app = FastAPI()
templates = Jinja2Templates(directory="templates")

# MCPæœåŠ¡å™¨é…ç½®
MCP_SERVER_URL = "http://localhost:8090/sse"

def build_messages(query: str, tool_results: list[dict]) -> list[dict]:
    """
    æ„å»ºæ”¯æŒå¤šå·¥å…·è°ƒç”¨ã€åŠ¨æ€ä¸Šä¸‹æ–‡çš„æ¶ˆæ¯ç»“æ„
    :param query: åŸå§‹ç”¨æˆ·é—®é¢˜
    :param tool_results: å·¥å…·è°ƒç”¨ç»“æœåˆ—è¡¨ï¼Œæ ¼å¼ [{"tool_name": str, "content": str}]
    :return: ç¬¦åˆOpenAIæ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨
    """
    # 1. èšåˆå·¥å…·ç»“æœ
    result_content = "\n\n".join(
        f"ã€{res['tool_name']}ã€‘\n{res['content']}" 
        for res in tool_results
    )
    
    # 2. æ„å»ºç³»ç»ŸæŒ‡ä»¤æ¨¡æ¿
    system_directive = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šåŠ©æ‰‹ï¼Œéœ€ä¸¥æ ¼éµå¾ªï¼š\n"
        "1. ç›´æ¥åŸºäºå·¥å…·ç»“æœå›ç­”é—®é¢˜\n"
        "2. ç¦æ­¢å±•ç¤ºæ€è€ƒè¿‡ç¨‹\n"
        "3. å½“å·¥å…·ç»“æœå†²çªæ—¶æ ‡æ³¨çŸ›ç›¾ç‚¹\n"
        "---å·¥å…·è¿”å›ç»“æœ---\n"
        f"{result_content}"
    )
    
    # 3. ç”¨æˆ·æŒ‡ä»¤å¼ºåŒ–
    user_prompt = (
        f"é—®é¢˜ï¼š{query}\n"
        "è¦æ±‚ï¼š\n"
        "- ç”¨ç®€æ´çš„é™ˆè¿°å¥å›ç­”\n"
        "- å¦‚ä¿¡æ¯ä¸è¶³è¯·æ˜ç¡®è¯´æ˜\n"
        "/no_think"  # å¼ºåŒ–æŒ‡ä»¤æ ‡è®°
    )
    
    return [
        {"role": "system", "content": system_directive},
        {"role": "user", "content": user_prompt}
    ]
    
async def llm_driven_tool_call(query: str):
    # 1. è·å–æœåŠ¡ç«¯å·¥å…·åˆ—è¡¨
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            # åˆå§‹åŒ–ä¼šè¯è¿æ¥
            await session.initialize()
            print("ğŸ”Œ MCPä¼šè¯è¿æ¥æˆåŠŸ")
            
            # åŠ è½½å·¥å…·
            response = await session.list_tools()  # âš ï¸ æ³¨æ„ await
            tools = response.tools  # æå–å·¥å…·å¯¹è±¡åˆ—è¡¨
            print("å¯ç”¨å·¥å…·:", [tool.name for tool in tools])  # æ‰“å°å·¥å…·åç§°[1,3](@ref)
            tools_params = [{
                    "type": "function",
                    "function": {
                        "name": tool.name,
                        "description": tool.description,
                        "parameters": tool.inputSchema  # ä½¿ç”¨ JSON Schema çº¦æŸå‚æ•°
                    }
                } for tool in tools]
            print("tools_params", tools_params)
            
            # 2. LLM å†³ç­–è°ƒç”¨å“ªä¸ªå·¥å…·åŠå‚æ•°
            try:
                llm_response = ollama_llm.chat.completions.create(
                    model="qwen3:4b",  # ç§»é™¤æ¨¡å‹åç§°åçš„ç©ºæ ¼
                    temperature=0.1,
                    messages=[{"role": "user", "content": f"{query}" }],
                    stream=False,  # ç¦ç”¨æµå¼è¾“å‡º
                    max_tokens=4096,              # ä¸Šä¸‹æ–‡é•¿åº¦
                    tools=tools_params
                )
                
                print("llm_response",llm_response)
                
                # 3. è§£æ LLM çš„è°ƒç”¨æŒ‡ä»¤
                tool_calls = llm_response.choices[0].message.tool_calls
                tools_results = []
                for tool_call in tool_calls:
                    tool_name = tool_call.function.name
                    tool_args = json.loads(tool_call.function.arguments)
                    print(f"è°ƒç”¨å·¥å…·: {tool_name}, å‚æ•°: {tool_args}")
                    
                    # 4. æ‰§è¡Œå·¥å…·è°ƒç”¨å¹¶æ”¶é›†ç»“æœ
                    result = await session.call_tool(tool_name, tool_args)
                    tools_results.append({"tool_name": tool_name, "content": result.content[0].text})  # å­˜å‚¨æ¯ä¸ªå·¥å…·çš„ç»“æœ

                print("æ‰€æœ‰å·¥å…·è°ƒç”¨ç»“æœ:", tools_results)
                messages = build_messages(query,tools_results)
            except Exception as e:
                print(str(e)) 
                tools_results = []
                messages = build_messages(query,tools_results)
            # 5. å°†ç»“æœè¿”å›ç»™ LLM ç”Ÿæˆæœ€ç»ˆå›ç­”f
            llm_response = ollama_llm.chat.completions.create(
                model="qwen3:4b",
                max_tokens=4096,
                messages=messages,
            )
            return llm_response.choices[0].message.content
        
@app.get("/", response_class=HTMLResponse)
async def chat_page(request: Request):
    """æ¸²æŸ“èŠå¤©ç•Œé¢"""
    return templates.TemplateResponse("chat.html", {"request": request})

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """WebSocketé€šä¿¡å¤„ç†"""
    await websocket.accept()
    while True:
        # æ¥æ”¶ç”¨æˆ·æ¶ˆæ¯
        # print("client",client)
        user_input = await websocket.receive_text()
        
        # è°ƒç”¨MCPå·¥å…·
        # Broken Code (tuple error)
        response = await llm_driven_tool_call(user_input)
        # response = await client.call_tool("ai_chat",parameters={"query": user_input})
        
        
        pattern = r'<think>.*?</think>'
        response = re.sub(pattern, '', response, flags=re.DOTALL)
        
        # todo å‘é€MCPå“åº” æ„å»ºmcp 
        print("AIå›å¤",response)
        await websocket.send_text(f"AIå›å¤: {response}")


from uvicorn import Server
class ProactorServer(Server):
    def run(self, sockets=None):
        loop = asyncio.ProactorEventLoop()  # æ˜¾å¼æŒ‡å®šäº‹ä»¶å¾ªç¯
        asyncio.set_event_loop(loop)
        super().run(sockets=sockets)

if __name__ == "__main__":
    import uvicorn
    # uvicorn.run(app, host="0.0.0.0", port=8000) # linux 
    config = uvicorn.Config("app:app", host="0.0.0.0", port=8000, reload=False)
    server = ProactorServer(config)
    server.run()